"""
å®‰å…¨æ— å¾ªç¯å¼•ç”¨çš„ nnUNet Trainer with TE-Swin UNet3D architecture.
ä¸»è¦ä¿®å¤ï¼šç§»é™¤å¯èƒ½å¯¼è‡´å¾ªç¯å¼•ç”¨çš„ä»£ç ï¼Œç®€åŒ–é”™è¯¯å¤„ç†
"""
import torch
import torch.nn as nn
from typing import Tuple, Union, List
from tqdm import tqdm
from dynamic_network_architectures.building_blocks.helper import get_matching_instancenorm, convert_dim_to_conv_op
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from nnunetv2.utilities.plans_handling.plans_handler import ConfigurationManager, PlansManager

# Import our custom model
from nnunetv2.training.nnUNetTrainer.variants.network_architecture.te_swin_models.nnUNet_TE_SwinUnet3D import (
    create_te_swinunet_s_3d, 
    create_te_swinunet_t_3d, 
    create_te_swinunet_b_3d
)


class nnUNetTrainer_TE_SwinUnet3D(nnUNetTrainer):
    """
    å®‰å…¨ç‰ˆæœ¬çš„ nnUNet Trainer using TE-Swin UNet3D architecture.
    """
    
    def __init__(self, plans: dict, configuration: str, fold: int, dataset_json: dict, 
                 device: torch.device = torch.device('cuda')):
        """Initialize TE-Swin UNet3D trainer."""
        
        # âœ… ç«‹å³ç¦ç”¨torch.compileï¼Œé˜²æ­¢åŠ¨æ€å°ºå¯¸é—®é¢˜
        import os
        import torch._dynamo
        os.environ['NNUNET_COMPILE'] = '0'
        torch._dynamo.config.suppress_errors = True
        print("ğŸ”§ Disabled torch.compile for TE-Swin UNet3D compatibility")
        
        super().__init__(plans=plans, configuration=configuration, fold=fold, 
                        dataset_json=dataset_json, device=device)
        
        # Model variant - ä½¿ç”¨tinyå˜ä½“ï¼Œå‚æ•°æ›´å°‘ï¼Œæ›´ç¨³å®š
        self.model_variant = 't'
        
    def build_network_architecture(self, architecture_class_name: str,
                                   arch_init_kwargs: dict,
                                   arch_init_kwargs_req_import: Union[List[str], Tuple[str, ...]],
                                   num_input_channels: int,
                                   num_output_channels: int,
                                   enable_deep_supervision: bool = True):
        """ç½‘ç»œæ¶æ„æ„å»ºæ–¹æ³•"""
        self.print_to_log_file(f"Building TE-SwinUnet3D-{self.model_variant} variant")
        self.print_to_log_file(f"Input channels: {num_input_channels}, Output channels: {num_output_channels}")
        self.print_to_log_file(f"Deep supervision: {enable_deep_supervision}")
        
        # âœ… ä½¿ç”¨å…¼å®¹æ€§ä¼˜åŒ–çš„å‚æ•°
        common_params = {
            'input_channels': num_input_channels,
            'num_classes': num_output_channels,
            'deep_supervision': enable_deep_supervision,
            'window_size': 4,  # å…¼å®¹æ€§ä¼˜åŒ–
            'downscaling_factors': (2, 2, 2, 2)  # å…¼å®¹æ€§ä¼˜åŒ–
        }
        
        if self.model_variant == 't':
            network = create_te_swinunet_t_3d(**common_params)
        elif self.model_variant == 'b':
            network = create_te_swinunet_b_3d(**common_params)
        else:  # Default to 's' (small) model
            network = create_te_swinunet_s_3d(**common_params)
            
        self.print_to_log_file(f"âœ… Successfully created TE-SwinUnet3D-{self.model_variant}")
        self.print_to_log_file(f"Model parameters: {sum(p.numel() for p in network.parameters()):,}")
        
        # âœ… ç®€åŒ–çš„è®¾å¤‡ç§»åŠ¨
        current_device = next(network.parameters()).device
        if current_device != self.device:
            self.print_to_log_file(f"Moving model from {current_device} to {self.device}")
            network = network.to(self.device)
            self.print_to_log_file(f"âœ… Model moved to device: {self.device}")
        else:
            self.print_to_log_file(f"âœ… Model already on correct device: {self.device}")
            
        # âœ… éªŒè¯nnUNetå…¼å®¹æ€§å±æ€§
        if hasattr(network, 'decoder'):
            self.print_to_log_file("âœ… Model has decoder attribute for nnUNet compatibility")
        else:
            self.print_to_log_file("âš ï¸  Model missing decoder attribute")
        
        return network
    
    def initialize(self):
        """é‡å†™åˆå§‹åŒ–æ–¹æ³•ï¼Œç¡®ä¿æ­£ç¡®çš„è®¾å¤‡ç®¡ç†"""
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().initialize()
        
        # âœ… ç¡®ä¿ç½‘ç»œåœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        if hasattr(self, 'network') and self.network is not None:
            self.network = self.network.to(self.device)
            self.print_to_log_file(f"âœ… Network confirmed on device: {next(self.network.parameters()).device}")
            
        # âœ… è®¾ç½®ç½‘ç»œä¸ºè®­ç»ƒæ¨¡å¼
        if hasattr(self, 'network'):
            self.network.train()
    
    def set_deep_supervision_enabled(self, enabled: bool):
        """æ·±åº¦ç›‘ç£è®¾ç½®æ–¹æ³•"""
        # è·å–å®é™…çš„æ¨¡å‹ï¼ˆå¤„ç†DDPå’ŒcompileåŒ…è£…ï¼‰
        if self.is_ddp:
            mod = self.network.module
        else:
            mod = self.network
            
        # å¤„ç†torch.compileåŒ…è£…
        if hasattr(mod, '_orig_mod'):
            mod = mod._orig_mod
        
        # âœ… å¤šç§æ–¹å¼å°è¯•è®¾ç½®æ·±åº¦ç›‘ç£
        success = False
        
        # æ–¹å¼1ï¼šé€šè¿‡decoderè®¾ç½®ï¼ˆnnUNetæ ‡å‡†æ–¹å¼ï¼‰
        if hasattr(mod, 'decoder') and hasattr(mod.decoder, 'deep_supervision'):
            mod.decoder.deep_supervision = enabled
            self.print_to_log_file(f"âœ… Set via decoder.deep_supervision: {enabled}")
            success = True
        
        # æ–¹å¼2ï¼šç›´æ¥è®¾ç½®æ¨¡å‹å±æ€§
        if hasattr(mod, 'deep_supervision'):
            mod.deep_supervision = enabled
            self.print_to_log_file(f"âœ… Set via model.deep_supervision: {enabled}")
            success = True
        
        # æ–¹å¼3ï¼šè®¾ç½®do_dsæ ‡å¿—
        if hasattr(mod, 'do_ds'):
            mod.do_ds = enabled
            self.print_to_log_file(f"âœ… Set via model.do_ds: {enabled}")
            success = True
            
        if not success:
            self.print_to_log_file(f"âš ï¸  Could not set deep supervision - continuing anyway")
            
    def on_train_start(self):
        """è®­ç»ƒå¼€å§‹æ—¶çš„è®¾å¤‡æ£€æŸ¥"""
        # âœ… åŸºæœ¬çš„è®¾å¤‡çŠ¶æ€æ£€æŸ¥
        if hasattr(self, 'network') and self.network is not None:
            network_device = next(self.network.parameters()).device
            self.print_to_log_file(f"ğŸ” Network device: {network_device}")
            self.print_to_log_file(f"ğŸ” Expected device: {self.device}")
            
            if network_device != self.device:
                self.print_to_log_file(f"âš ï¸  Device mismatch detected!")
                    
        # âœ… GPUå†…å­˜çŠ¶æ€æ£€æŸ¥
        if torch.cuda.is_available():
            self.print_to_log_file(f"ğŸ” GPU memory allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB")
            self.print_to_log_file(f"ğŸ” GPU memory reserved: {torch.cuda.memory_reserved()/1024**3:.2f} GB")
        
        print("ğŸš€ Starting TE-Swin UNet3D training with 'MRI as GIF' approach...")
        
        # âœ… è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        super().on_train_start()
            
    def train_step(self, batch):
        """ä¿®å¤çš„è®­ç»ƒæ­¥éª¤ - å¼ºåˆ¶ç»Ÿä¸€å°ºå¯¸å¤„ç†"""
        # âœ… æ­£ç¡®å¤„ç†æ•°æ® - å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å¼ é‡
        data = batch['data']
        target = batch['target']
        
        # å¤„ç†æ•°æ®è®¾å¤‡ç§»åŠ¨
        if isinstance(data, list):
            data = [d.to(self.device, non_blocking=True) if hasattr(d, 'to') else d for d in data]
        else:
            if hasattr(data, 'device') and data.device != self.device:
                data = data.to(self.device, non_blocking=True)
                
        # å¤„ç†ç›®æ ‡è®¾å¤‡ç§»åŠ¨  
        if isinstance(target, list):
            target = [t.to(self.device, non_blocking=True) if hasattr(t, 'to') else t for t in target]
        else:
            if hasattr(target, 'device') and target.device != self.device:
                target = target.to(self.device, non_blocking=True)
        
        # âœ… å¼ºåˆ¶æ£€æŸ¥å¹¶ä¿®å¤å°ºå¯¸ä¸åŒ¹é…
        if hasattr(data, 'shape') and hasattr(target, 'shape') and len(data.shape) >= 3 and len(target.shape) >= 2:
            data_spatial = data.shape[2:]  # D, H, W
            
            # targetå¯èƒ½æ˜¯[B, D, H, W]æˆ–[B, C, D, H, W]æ ¼å¼
            if len(target.shape) == 4:  # [B, D, H, W]
                target_spatial = target.shape[1:]
            elif len(target.shape) == 5:  # [B, C, D, H, W] 
                target_spatial = target.shape[2:]
            else:
                target_spatial = None
                
            if target_spatial is not None and data_spatial != target_spatial:
                self.print_to_log_file(f"ğŸ”§ Detected size mismatch: data {data_spatial} vs target {target_spatial}")
                
                # è®¡ç®—éœ€è¦çš„padding
                pad_d = max(0, data_spatial[0] - target_spatial[0])
                pad_h = max(0, data_spatial[1] - target_spatial[1])
                pad_w = max(0, data_spatial[2] - target_spatial[2])
                
                if pad_d > 0 or pad_h > 0 or pad_w > 0:
                    # PyTorch paddingæ ¼å¼: (W_left, W_right, H_top, H_bottom, D_front, D_back)
                    padding = (0, pad_w, 0, pad_h, 0, pad_d)
                    target = torch.nn.functional.pad(target, padding, mode='constant', value=0)
                    self.print_to_log_file(f"ğŸ”§ Padded target to {target.shape}")
                    
        # æœ€ç»ˆå°ºå¯¸æ£€æŸ¥å’Œæ—¥å¿—
        if not hasattr(self, '_logged_final_shapes'):
            if hasattr(data, 'shape'):
                self.print_to_log_file(f"ğŸ” Final data shape: {data.shape}")
            if hasattr(target, 'shape'):
                self.print_to_log_file(f"ğŸ” Final target shape: {target.shape}")
            self._logged_final_shapes = True
                
        batch['data'] = data
        batch['target'] = target
        
        # è°ƒç”¨çˆ¶ç±»çš„è®­ç»ƒæ­¥éª¤
        return super().train_step(batch)

    def validation_step(self, batch):
        """ä¿®å¤çš„éªŒè¯æ­¥éª¤ - æ­£ç¡®å¤„ç†æ•°æ®æ ¼å¼"""
        # âœ… æ­£ç¡®å¤„ç†æ•°æ® - å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å¼ é‡
        data = batch['data']
        target = batch['target']
        
        # å¤„ç†æ•°æ®è®¾å¤‡ç§»åŠ¨
        if isinstance(data, list):
            data = [d.to(self.device, non_blocking=True) if hasattr(d, 'to') else d for d in data]
        else:
            if hasattr(data, 'device') and data.device != self.device:
                data = data.to(self.device, non_blocking=True)
                
        # å¤„ç†ç›®æ ‡è®¾å¤‡ç§»åŠ¨
        if isinstance(target, list):
            target = [t.to(self.device, non_blocking=True) if hasattr(t, 'to') else t for t in target]
        else:
            if hasattr(target, 'device') and target.device != self.device:
                target = target.to(self.device, non_blocking=True)
                
        batch['data'] = data
        batch['target'] = target
        
        return super().validation_step(batch)


class nnUNetTrainer_TE_SwinUnet3D_tiny(nnUNetTrainer_TE_SwinUnet3D):
    """Tiny variant - æœ€å®‰å…¨çš„é€‰æ‹©"""
    
    def __init__(self, plans: dict, configuration: str, fold: int, dataset_json: dict, 
                 device: torch.device = torch.device('cuda')):
        super().__init__(plans=plans, configuration=configuration, fold=fold, 
                         dataset_json=dataset_json, device=device)
        self.model_variant = 't'


class nnUNetTrainer_TE_SwinUnet3D_small(nnUNetTrainer_TE_SwinUnet3D):
    """Small variant"""
    
    def __init__(self, plans: dict, configuration: str, fold: int, dataset_json: dict, 
                 device: torch.device = torch.device('cuda')):
        super().__init__(plans=plans, configuration=configuration, fold=fold, 
                         dataset_json=dataset_json, device=device)
        self.model_variant = 's'


class nnUNetTrainer_TE_SwinUnet3D_base(nnUNetTrainer_TE_SwinUnet3D):
    """Base variant"""
    
    def __init__(self, plans: dict, configuration: str, fold: int, dataset_json: dict, 
                 device: torch.device = torch.device('cuda')):
        super().__init__(plans=plans, configuration=configuration, fold=fold, 
                         dataset_json=dataset_json, device=device)
        self.model_variant = 'b'